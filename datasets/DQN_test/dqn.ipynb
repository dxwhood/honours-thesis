{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dim: 500, output_dim: 6\n",
      "Episode 0 finished.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (1000, 5) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/student/d2ql/honours-thesis/datasets/DQN_test/dqn.ipynb Cell 1\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu1/home/student/d2ql/honours-thesis/datasets/DQN_test/dqn.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=94'>95</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minput_dim: \u001b[39m\u001b[39m{\u001b[39;00minput_dim\u001b[39m}\u001b[39;00m\u001b[39m, output_dim: \u001b[39m\u001b[39m{\u001b[39;00moutput_dim\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu1/home/student/d2ql/honours-thesis/datasets/DQN_test/dqn.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=95'>96</a>\u001b[0m model \u001b[39m=\u001b[39m DQN(input_dim, output_dim)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu1/home/student/d2ql/honours-thesis/datasets/DQN_test/dqn.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=96'>97</a>\u001b[0m train_dqn(env, model)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu1/home/student/d2ql/honours-thesis/datasets/DQN_test/dqn.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=98'>99</a>\u001b[0m \u001b[39m#save model\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu1/home/student/d2ql/honours-thesis/datasets/DQN_test/dqn.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=99'>100</a>\u001b[0m torch\u001b[39m.\u001b[39msave(model, \u001b[39m'\u001b[39m\u001b[39mdqn-model1.pt\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/home/student/d2ql/honours-thesis/datasets/DQN_test/dqn.ipynb Cell 1\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu1/home/student/d2ql/honours-thesis/datasets/DQN_test/dqn.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39m# Train using experience replay\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu1/home/student/d2ql/honours-thesis/datasets/DQN_test/dqn.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(replay_buffer) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m MIN_REPLAY_BUFFER_SIZE:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu1/home/student/d2ql/honours-thesis/datasets/DQN_test/dqn.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m     batch \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mchoice(a\u001b[39m=\u001b[39;49m\u001b[39mlist\u001b[39;49m(replay_buffer), size\u001b[39m=\u001b[39;49mBATCH_SIZE)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu1/home/student/d2ql/honours-thesis/datasets/DQN_test/dqn.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m     states, actions, rewards, next_states, dones \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu1/home/student/d2ql/honours-thesis/datasets/DQN_test/dqn.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m     states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(states, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\n",
      "File \u001b[0;32mnumpy/random/mtrand.pyx:936\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (1000, 5) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gym\n",
    "from collections import deque\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "def one_hot_state(state, n_states):\n",
    "    vec = np.zeros(n_states)\n",
    "    vec[state] = 1\n",
    "    return vec\n",
    "\n",
    "# Constants\n",
    "REPLAY_BUFFER_SIZE = 10000\n",
    "MIN_REPLAY_BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Initialize replay buffer\n",
    "replay_buffer = deque(maxlen=10000)\n",
    "\n",
    "def train_dqn(env, model, episodes=500, learning_rate=0.001, discount_factor=0.95, exploration_prob=1.0, exploration_decay=0.995, min_exploration=0.05):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    n_states = env.observation_space.n\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = one_hot_state(env.reset()[0], n_states)\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        while not terminated and not truncated:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.tensor([state], dtype=torch.float32)\n",
    "                if np.random.uniform(0, 1) < exploration_prob:\n",
    "                    action = env.action_space.sample()  # Explore\n",
    "                else:\n",
    "                    q_values = model(state_tensor)\n",
    "                    action = torch.argmax(q_values).item()  # Exploit\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            next_state = one_hot_state(next_state, n_states)\n",
    "\n",
    "            # Store experience in replay buffer\n",
    "            replay_buffer.append((state, action, reward, next_state, terminated))\n",
    "            if len(replay_buffer) > REPLAY_BUFFER_SIZE:\n",
    "                replay_buffer.pop(0)  # Remove oldest experience if buffer is full\n",
    "\n",
    "            # Train using experience replay\n",
    "            if len(replay_buffer) >= MIN_REPLAY_BUFFER_SIZE:\n",
    "                batch = sample(replay_buffer, BATCH_SIZE)\n",
    "                states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "                states = torch.tensor(states, dtype=torch.float32)\n",
    "                actions = torch.tensor(actions, dtype=torch.long)\n",
    "                rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "                next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "                dones = torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "                q_values = model(states)\n",
    "                q_values = q_values.gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    next_q_values = model(next_states)\n",
    "                    next_q_values = torch.max(next_q_values, dim=1)[0]\n",
    "                    targets = rewards + (1 - dones) * discount_factor * next_q_values\n",
    "\n",
    "                loss = loss_fn(q_values, targets)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        exploration_prob = max(min_exploration, exploration_prob * exploration_decay)\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episode {episode} finished.\")\n",
    "\n",
    "\n",
    "# Train\n",
    "env = gym.make('Taxi-v3')\n",
    "input_dim = env.observation_space.n  # Adjusted to match the number of states\n",
    "output_dim = env.action_space.n\n",
    "print(f\"input_dim: {input_dim}, output_dim: {output_dim}\")\n",
    "model = DQN(input_dim, output_dim)\n",
    "train_dqn(env, model)\n",
    "\n",
    "#save model\n",
    "torch.save(model, 'dqn-model1.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: 500\n",
      "Debug - next_state: 134\n",
      "Debug - state: [134]\n",
      "Debug - action: 4\n",
      "Debug - target: tensor(108.1784)\n",
      "Debug - next_state: 134\n",
      "Debug - state: 134\n",
      "Debug - action: 4\n",
      "Debug - target: tensor(108.0502)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of bounds for dimension 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/student/d2ql/honours-thesis/datasets/DQN_test/dqn.ipynb Cell 6\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu1/home/student/d2ql/honours-thesis/datasets/DQN_test/dqn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m output_dim \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mn\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu1/home/student/d2ql/honours-thesis/datasets/DQN_test/dqn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m model \u001b[39m=\u001b[39m DQN(input_dim, output_dim)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu1/home/student/d2ql/honours-thesis/datasets/DQN_test/dqn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m train_dqn(env, model)\n",
      "\u001b[1;32m/home/student/d2ql/honours-thesis/datasets/DQN_test/dqn.ipynb Cell 6\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu1/home/student/d2ql/honours-thesis/datasets/DQN_test/dqn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDebug - target:\u001b[39m\u001b[39m\"\u001b[39m, target)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu1/home/student/d2ql/honours-thesis/datasets/DQN_test/dqn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m q_values \u001b[39m=\u001b[39m model(torch\u001b[39m.\u001b[39mtensor([state], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32))\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu1/home/student/d2ql/honours-thesis/datasets/DQN_test/dqn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(q_values[\u001b[39m0\u001b[39;49m][action], torch\u001b[39m.\u001b[39mtensor([target], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu1/home/student/d2ql/honours-thesis/datasets/DQN_test/dqn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu1/home/student/d2ql/honours-thesis/datasets/DQN_test/dqn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "\u001b[0;31mIndexError\u001b[0m: index 4 is out of bounds for dimension 0 with size 0"
     ]
    }
   ],
   "source": [
    "def train_dqn(env, model, episodes=5000, learning_rate=0.001, discount_factor=0.95, exploration_prob=1.0, exploration_decay=0.995, min_exploration=0.05):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        state = np.array([state])\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        while not terminated and not truncated:\n",
    "            with torch.no_grad():\n",
    "                if np.random.uniform(0, 1) < exploration_prob:\n",
    "                    action = env.action_space.sample()  # Explore\n",
    "                else:\n",
    "                    q_values = model(torch.tensor([state], dtype=torch.float32))\n",
    "                    print(\"Debug - q_values shape:\", q_values.shape)\n",
    "                    print(\"Debug - q_values:\", q_values)\n",
    "                    action = torch.argmax(q_values).item()  # Exploit\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            print(\"Debug - next_state:\", next_state)\n",
    "            print(\"Debug - state:\", state)\n",
    "            print(\"Debug - action:\", action)\n",
    "\n",
    "            # Update model\n",
    "            target = reward\n",
    "            if not terminated and not truncated:\n",
    "                with torch.no_grad():\n",
    "                    target = reward + discount_factor * torch.max(model(torch.tensor([next_state], dtype=torch.float32)))\n",
    "                    print(\"Debug - target:\", target)\n",
    "\n",
    "            q_values = model(torch.tensor([state], dtype=torch.float32))\n",
    "           \n",
    "            loss = loss_fn(q_values[0][action], torch.tensor([target], dtype=torch.float32))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        exploration_prob = max(min_exploration, exploration_prob * exploration_decay)\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode {episode} finished.\")\n",
    "\n",
    "# Train\n",
    "env = gym.make('Taxi-v3')\n",
    "print(\"Observation space:\", env.observation_space.n)\n",
    "input_dim = 1\n",
    "output_dim = env.action_space.n\n",
    "model = DQN(input_dim, output_dim)\n",
    "train_dqn(env, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
