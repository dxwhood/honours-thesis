{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports/Setup and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import math\n",
    "import time\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int2analog(x, n=10):\n",
    "    # Convert an integer to a PyTorch tensor\n",
    "    x_tensor = torch.tensor([x], dtype=torch.int32)\n",
    "\n",
    "    # Convert integers into the corresponding binary bits.\n",
    "    shifts = torch.arange(n - 1, -1, -1, dtype=x_tensor.dtype)\n",
    "    x_tensor = torch.bitwise_right_shift(x_tensor, shifts)\n",
    "    x_tensor = torch.remainder(x_tensor, 2)\n",
    "\n",
    "    # Convert the binary bits into the corresponding analog values.\n",
    "    x_tensor = x_tensor.type(torch.float32)\n",
    "    x_tensor = 2 * x_tensor - 1\n",
    "\n",
    "\n",
    "    return x_tensor  \n",
    "\n",
    "def analog2int(x):\n",
    "    # Convert an analog bit representation back to an integer\n",
    "    x = (x + 1) / 2  # Convert from [-1, 1] to [0, 1]\n",
    "    x = torch.round(x).type(torch.int32)  # Round and convert to int\n",
    "    # Convert binary bits back to integer\n",
    "    int_val = 0\n",
    "    for i, bit in enumerate(reversed(x)):\n",
    "        int_val += bit.item() * (2 ** i)\n",
    "    return int_val\n",
    "\n",
    "\n",
    "def visualize_sequence(sequence, title=\"\"):\n",
    "    \"\"\"\n",
    "    Visualizes a sequence as an image.\n",
    "\n",
    "    Args:\n",
    "    - sequence (torch.Tensor): A tensor of shape (9,) representing a sequence.\n",
    "    - title (str): Title of the image.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert sequence to a numpy array\n",
    "    if isinstance(sequence, torch.Tensor):\n",
    "        sequence = sequence.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "    # Reshape sequence to (1, 9) and transpose it to (9, 1)\n",
    "    sequence = sequence.reshape((9, 1)).T\n",
    "\n",
    "    # Map noisy sequences from [-1, 1] to [0, 1] and clip\n",
    "    sequence = np.clip(sequence, -1, 1)\n",
    "\n",
    "    # Plot the noisy sequences\n",
    "    plt.figure(figsize=(9, 1))\n",
    "    plt.imshow(sequence, cmap=\"gray\", vmin=0, vmax=1)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#visualize 10 noisy sequences in a grid\n",
    "def visualize_noisy_sequences(noisy_sequences, title=\"\"):\n",
    "    \"\"\"\n",
    "    Visualizes a list of noisy sequences as a 10x9 grid of images. Maps sequences from [-1, 1] to [0, 1] and clips values out of bounds.\n",
    "    \"\"\"\n",
    "    # Convert noisy sequences to a numpy array\n",
    "    noisy_sequences = [seq.numpy() for seq in noisy_sequences]\n",
    "\n",
    "    # Reshape noisy sequences to a 10x9 grid\n",
    "    noisy_sequences = np.array(noisy_sequences).reshape((10, 9))\n",
    "\n",
    "    # Map noisy sequences from [-1, 1] to [0, 1] and clip\n",
    "    #noisy_sequences = np.clip(noisy_sequences, -1, 1)\n",
    "\n",
    "    #determine vmin and vmax\n",
    "    vmin = np.min(noisy_sequences)\n",
    "    vmax = np.max(noisy_sequences)\n",
    "    # Plot the noisy sequences\n",
    "    plt.figure(figsize=(9, 10))\n",
    "    plt.imshow(noisy_sequences, cmap=\"gray\", vmin=-1, vmax=1)\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxi Dataset & Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape:  (65544, 5)\n",
      "Sample data:  [23.  3. -1.  3.  0.]\n",
      "Analog Dataset len:  65544\n",
      "Sample analog bit data:  [tensor([-1., -1., -1., -1.,  1., -1.,  1.,  1.,  1.]), tensor([-1.,  1.,  1.]), tensor(-1.), tensor([-1., -1., -1., -1., -1., -1., -1.,  1.,  1.]), tensor(0.)]\n"
     ]
    }
   ],
   "source": [
    "path = 'taxi_q_expert_dataset.csv'\n",
    "\n",
    "# Dataset format:\n",
    "# (state, action, reward, next_state, done)\n",
    "\n",
    "# Load the dataset into torch (remove the header line and convert true/false to 0/1)\n",
    "data = np.loadtxt(path, delimiter=',', skiprows=1, converters={4: lambda x: int(x == 'True')})\n",
    "\n",
    "print(\"Dataset Shape: \", data.shape)\n",
    "print(\"Sample data: \", data[1])\n",
    "\n",
    "#function to convert the data into analog bits (500 states so we need 9 bits)\n",
    "# Ex. [5. 1. -1. 3. 0.] -> [[-1. -1. -1. -1. -1. -1. 1. -1. 1.], [-1. -1. -1. -1. -1. -1. -1. 1. 1.], -1., [-1. -1. -1. -1. -1. -1. -1. 1. 1.], 0.]\n",
    "\n",
    "def convert_data(data):\n",
    "    #convert the data into analog bits\n",
    "    analog_data = []\n",
    "    for i in range(data.shape[0]):\n",
    "        analog_state = []\n",
    "        for j in range(data.shape[1]):\n",
    "            #if action only use 3 bits (6 possible actions)\n",
    "            if j == 1:\n",
    "                analog_state.append(int2analog(data[i][j], 3))\n",
    "            else:\n",
    "                analog_state.append(int2analog(data[i][j], 9))\n",
    "        analog_data.append(analog_state)\n",
    "\n",
    "    #convert the reward and done back to int\n",
    "    for i in range(data.shape[0]):\n",
    "        analog_data[i][2] = torch.tensor(data[i][2], dtype=torch.float32)\n",
    "        analog_data[i][4] = torch.tensor(data[i][4], dtype=torch.float32)\n",
    "        \n",
    "    return analog_data\n",
    "\n",
    "analog_data = convert_data(data)\n",
    "# change from list to appropriate format\n",
    "\n",
    "\n",
    "print(\"Analog Dataset len: \", len(analog_data))\n",
    "print(\"Sample analog bit data: \", analog_data[1])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State batch shape:  torch.Size([32, 9])\n",
      "Action batch shape:  torch.Size([32, 3])\n",
      "Reward batch shape:  torch.Size([32])\n",
      "Next State batch shape:  torch.Size([32, 9])\n",
      "Done batch shape:  torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "class AnalogDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        state, action, reward, next_state, done = self.data[idx]\n",
    "        return state, action, reward, next_state, done\n",
    "\n",
    "\n",
    "# Create the dataset\n",
    "dataset = AnalogDataset(analog_data)\n",
    "\n",
    "# from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "#test the dataloader\n",
    "for state, action, reward, next_state, done in dataloader:\n",
    "    print(\"State batch shape: \", state.shape)\n",
    "    print(\"Action batch shape: \", action.shape)\n",
    "    print(\"Reward batch shape: \", reward.shape)\n",
    "    print(\"Next State batch shape: \", next_state.shape)\n",
    "    print(\"Done batch shape: \", done.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, bit_dim, time_embedding_dim, hidden_dim=64):\n",
    "        super(MLP, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.bit_dim = bit_dim\n",
    "        self.time_embedding_dim = time_embedding_dim\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        #if torch.backends.mps.is_available(): self.device = torch.device(\"mps\")\n",
    "\n",
    "        \n",
    "\n",
    "        # Time Embedding Layer\n",
    "        self.time_embedding = SinusoidalPosEmb(time_embedding_dim)\n",
    "\n",
    "        # Input dimension: state dimension + bit-encoded action dimension + time embedding dimension\n",
    "        input_dim = state_dim + bit_dim + time_embedding_dim\n",
    "\n",
    "        # Define the MLP layers\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.Mish(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Mish(),\n",
    "            nn.Linear(hidden_dim, bit_dim)  # Output dimension is the bit-encoded action dimension\n",
    "        )\n",
    "\n",
    "    def forward(self, state, time, bit_action):\n",
    "        # Generate time embedding\n",
    "        time_emb = self.time_embedding(time)\n",
    "\n",
    "        # Reshape time_emb to match the batch size of state and bit_action\n",
    "        time_emb = time_emb.expand(state.size(0), -1)\n",
    "\n",
    "        # Concatenate state, time embedding, and bit-encoded action\n",
    "        x = torch.cat([state, time_emb, bit_action], dim=-1)\n",
    "\n",
    "        # Pass through the MLP layers\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion Process\n",
    "The DDPM paper describes a corruption process that adds a small amount of noise for every 'timestep'. Given $x_{t-1}$ for some timestep, we can get the next (slightly more noisy) version $x_t$ with:<br><br>\n",
    "\n",
    "$q(\\mathbf{x}_t \\vert \\mathbf{x}_{t-1}) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{1 - \\beta_t} \\mathbf{x}_{t-1}, \\beta_t\\mathbf{I}) \\quad\n",
    "q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_0) = \\prod^T_{t=1} q(\\mathbf{x}_t \\vert \\mathbf{x}_{t-1})$<br><br>\n",
    "\n",
    "\n",
    "That is, we take $x_{t-1}$, scale it by $\\sqrt{1 - \\beta_t}$ and add noise scaled by $\\beta_t$. This $\\beta$ is defined for every t according to some schedule, and determines how much noise is added per timestep. To save time, there is a faster formula that computes all the timesteps simultaneously: <br><br>\n",
    "\n",
    "$\\begin{aligned}\n",
    "q(\\mathbf{x}_t \\vert \\mathbf{x}_0) &= \\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0, {(1 - \\bar{\\alpha}_t)} \\mathbf{I})\n",
    "\\end{aligned}$ where $\\bar{\\alpha}_t = \\prod_{i=1}^T \\alpha_i$ and $\\alpha_i = 1-\\beta_i$<br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_beta_schedule(T, s=0.1):\n",
    "    \"\"\"\n",
    "    Generate a cosine beta schedule for T timesteps.\n",
    "\n",
    "    Args:\n",
    "    - T (int): The number of timesteps in the diffusion process.\n",
    "    - s (float): A hyperparameter controlling the sharpness of the cosine curve. Typical values are between 0.001 and 0.1.\n",
    "\n",
    "    Returns:\n",
    "    - betas (torch.Tensor): A tensor of beta values for the diffusion schedule.\n",
    "    \"\"\"\n",
    "    # Define the steps\n",
    "    steps = torch.arange(0, T, dtype=torch.float32) / T\n",
    "\n",
    "    # Calculate the alphas using the cosine schedule\n",
    "    alphas = torch.cos(((steps + s) / (1 + s)) * np.pi * 0.5) ** 2\n",
    "    alphas = alphas / alphas[0]\n",
    "\n",
    "    # Calculate beta values from alphas\n",
    "    betas = 1 - alphas[1:] / alphas[:-1]\n",
    "    \n",
    "    # Ensure that beta values are within a valid range\n",
    "    betas = torch.clip(betas, 0.0001, 0.9999)\n",
    "\n",
    "    if len(betas) < T:\n",
    "        last_beta = betas[-1]\n",
    "        betas = torch.cat([betas, last_beta.unsqueeze(0)])\n",
    "\n",
    "    #print(\"BETA SCHEDULE: \", betas)\n",
    "\n",
    "    return betas\n",
    "\n",
    "def compute_alpha_bar(beta_schedule):\n",
    "    alpha = 1. - beta_schedule\n",
    "    alpha_bar = torch.cumprod(alpha, dim=0)\n",
    "    return alpha, alpha_bar\n",
    "\n",
    "\n",
    "def apply_noise(x, timestep, beta_schedule):\n",
    "    \"\"\"\n",
    "    Applies noise to an action x at a specific timestep.\n",
    "\n",
    "    Args:\n",
    "    - x (torch.Tensor): The initial action tensor.\n",
    "    - timestep (int): The specific timestep at which to apply noise.\n",
    "    - beta_schedule (torch.Tensor): The beta schedule tensor.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: The noised version of the action at the specified timestep.\n",
    "    \"\"\"\n",
    "    # Compute alpha and alpha_bar\n",
    "    alpha, alpha_bar = compute_alpha_bar(beta_schedule)\n",
    "\n",
    "    # Add noise to the action at the specified timestep\n",
    "    epsilon = torch.randn_like(x)\n",
    "    xt = torch.sqrt(alpha_bar[timestep]) * x + torch.sqrt(1. - alpha_bar[timestep]) * epsilon\n",
    "\n",
    "    return xt\n",
    "\n",
    "\n",
    "def make_data_to_plot(T, beta_schedule=False, s=0):\n",
    "    #if beta_schedule is not a tensor, make one\n",
    "    if type(beta_schedule) == bool:\n",
    "        if beta_schedule == False:\n",
    "            beta_schedule = torch.linspace(1e-4, 0.02, T)\n",
    "        else:\n",
    "            beta_schedule = cosine_beta_schedule(T, s=s)\n",
    "\n",
    "    x_to_plot = []\n",
    "    xt = torch.tensor([-1., -1., -1., -1., 0. , 1. , 1. , 1. , 1.])\n",
    "   #0 tensor\n",
    "    #xt = torch.tensor([0., 0., 0., 0., 0. , 0. , 0. , 0. , 0.])  # Initial image\n",
    "\n",
    "    x_to_plot.append(xt)\n",
    "\n",
    "    for t in range(T):\n",
    "        xt = apply_noise(xt, t, beta_schedule)\n",
    "        #we want to keep T/10 timesteps\n",
    "        if t % (T/10) == 0 and t > 0:    \n",
    "            x_to_plot.append(xt)\n",
    "\n",
    "    return x_to_plot    \n",
    "\n",
    "\n",
    "    \n",
    "def reverse_diffusion(x_noisy, timestep, model, state, beta_schedule):\n",
    "    \"\"\"\n",
    "    Performs a single reverse diffusion step.\n",
    "\n",
    "    Args:\n",
    "    - x_noisy (torch.Tensor): The current noisy version of the action data.\n",
    "    - timestep (int): The current timestep in the reverse diffusion process.\n",
    "    - model (nn.Module): The neural network model used for denoising.\n",
    "    - state (torch.Tensor): The current state of the environment.\n",
    "    - beta_schedule (torch.Tensor): The beta schedule tensor.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: The denoised version of the action data after the reverse diffusion step.\n",
    "    \"\"\"\n",
    "    alpha, alpha_bar = compute_alpha_bar(beta_schedule)\n",
    "\n",
    "    # Convert timestep to a tensor\n",
    "    timestep_tensor = torch.tensor([timestep], dtype=torch.float32, device=model.device)\n",
    "\n",
    "    # Generate model output\n",
    "    model_output = model(x_noisy, timestep_tensor, state)\n",
    "\n",
    "    x_denoised = (x_noisy - torch.sqrt(1. - alpha_bar[timestep]) * model_output) / torch.sqrt(alpha_bar[timestep])\n",
    "    return x_denoised\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diffusion Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BitDiffusion(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, bit_dim, model, T=100):\n",
    "        super(BitDiffusion, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.bit_dim = bit_dim\n",
    "        self.model = model  # The neural network for denoising\n",
    "        self.beta_schedule = cosine_beta_schedule(T)  # Define T as needed\n",
    "        #get mps or cpu or gpu\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        #if torch.backends.mps.is_available(): self.device = torch.device(\"mps\")\n",
    "\n",
    "        #print initiation of model\n",
    "        print(\"BitDiffusion Model Initiated with device \", self.device)\n",
    "\n",
    "\n",
    "    def forward_diffusion(self, x, t):\n",
    "        \"\"\"\n",
    "        Applies forward diffusion to the bit-encoded actions.\n",
    "\n",
    "        Args:\n",
    "        - x (torch.Tensor): The bit-encoded actions.\n",
    "        - t (int): The current timestep for diffusion.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: The noised version of the bit-encoded actions.\n",
    "        \"\"\"\n",
    "        return apply_noise(x, t, self.beta_schedule)\n",
    "\n",
    "    def reverse_diffusion(self, x_noisy, t, state):\n",
    "        \"\"\"\n",
    "        Performs reverse diffusion to denoise the bit-encoded actions.\n",
    "\n",
    "        Args:\n",
    "        - x_noisy (torch.Tensor): The current noisy version of the bit-encoded actions.\n",
    "        - t (int): The current timestep in the reverse diffusion process.\n",
    "        - state (torch.Tensor): The current state of the environment.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: The denoised version of the bit-encoded actions.\n",
    "        \"\"\"\n",
    "        return reverse_diffusion(x_noisy, t, self.model, state, self.beta_schedule)\n",
    "\n",
    "    def sample(self, state, shape):\n",
    "        \"\"\"\n",
    "        Generates a sample (bit-encoded action) given a state.\n",
    "\n",
    "        Args:\n",
    "        - state (torch.Tensor): The current state of the environment.\n",
    "        - shape (tuple): The shape of the tensor to generate.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: The generated bit-encoded action.\n",
    "        \"\"\"\n",
    "        batch_size = shape[0]\n",
    "        x = torch.randn(shape)  # Start with random noise\n",
    "        for t in reversed(range(0, self.beta_schedule.size(0))):\n",
    "            x = self.reverse_diffusion(x, t, state)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BitDiffusion Model Initiated with device  cpu\n"
     ]
    }
   ],
   "source": [
    "state_dim = 9  # Dimension of the state vector\n",
    "bit_dim = 3    # Dimension of the bit-encoded action vector\n",
    "action_dim = 6 # Dimension of the action vector\n",
    "hidden_dim = 64  # Number of neurons in each hidden layer\n",
    "time_embedding_dim = 8  # Dimension of the time embedding\n",
    "\n",
    "# Instantiate the MLP model\n",
    "mlp_model = MLP(state_dim, action_dim, bit_dim, time_embedding_dim, hidden_dim)\n",
    "\n",
    "# Instantiate the Diffusion model\n",
    "diffusion_model = BitDiffusion(state_dim, action_dim, bit_dim, mlp_model, T=50)\n",
    "\n",
    "batch_size = 256  # You can adjust this based on your computational resources\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "num_epochs = 1  # Number of epochs to train for\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Average Loss: 222.2662\n",
      "Training time: 0.51s\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(diffusion_model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "time_start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for state, bit_action, reward, next_state, done in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward diffusion step\n",
    "        timestep = torch.randint(0, diffusion_model.beta_schedule.size(0), (1,)).item()\n",
    "        noised_action = diffusion_model.forward_diffusion(bit_action, timestep)\n",
    "\n",
    "        # Reverse diffusion (denoising)\n",
    "        denoised_action = diffusion_model.reverse_diffusion(noised_action, timestep, state)\n",
    "\n",
    "        # Loss computation (MSE loss for behavior cloning)\n",
    "        loss = F.mse_loss(denoised_action, bit_action)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "time_end = time.time()\n",
    "print(f\"Training time: {time_end - time_start:.2f}s\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_action(model, state):\n",
    "    \"\"\"\n",
    "    Generate a bit-encoded action for a given state using the trained model.\n",
    "\n",
    "    Args:\n",
    "    - model (BitDiffusion): The trained diffusion model.\n",
    "    - state (torch.Tensor): The state tensor.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: The generated bit-encoded action.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Prepare the state tensor (ensure it's the right shape)\n",
    "        state = state.to(model.device).unsqueeze(0)  # Add batch dimension if necessary\n",
    "\n",
    "        # Generate a sample action\n",
    "        T = model.beta_schedule.size(0)  # Total number of timesteps\n",
    "        sample_shape = (1, model.bit_dim)\n",
    "        sampled_action = model.sample(state, sample_shape).squeeze(0)  # Remove batch dimension\n",
    "\n",
    "        return sampled_action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random State:  416\n",
      "Analog State:  tensor([ 1.,  1., -1.,  1., -1., -1., -1., -1., -1.])\n",
      "sampled action: tensor([-1.1299e+17, -2.1165e+16, -1.4571e+16])\n",
      "thresholded: tensor([-1., -1., -1.])\n",
      "int: 0\n",
      "Example: [416.   1.  -1. 316.   0.]\n"
     ]
    }
   ],
   "source": [
    "# get random state (discrete(500) space)\n",
    "\n",
    "rndm_state = torch.randint(0, 500, (1,)).item()\n",
    "\n",
    "print(\"Random State: \", rndm_state)\n",
    "#convert to analog\n",
    "analog_rndm_state = int2analog(rndm_state, 9)\n",
    "print(\"Analog State: \", analog_rndm_state)\n",
    "\n",
    "sampled_action = sample_action(diffusion_model, analog_rndm_state)\n",
    "\n",
    "print(f\"sampled action: {sampled_action}\")\n",
    "print(f\"thresholded: {torch.sign(sampled_action)}\")\n",
    "print(f\"int: {analog2int(torch.sign(sampled_action))}\")\n",
    "#taxi env meaning: \n",
    "case = {0: \"south\", 1: \"north\", 2: \"east\", 3: \"west\", 4: \"pickup\", 5: \"dropoff\"}\n",
    "#print(f\"Meaning: {case[analog2int(torch.sign(sampled_action))]}\")\n",
    "\n",
    "\n",
    "dataset = np.loadtxt(path, delimiter=',', skiprows=1, converters={4: lambda x: int(x == 'True')})\n",
    "\n",
    "#find the first 5 examples of the state\n",
    "first_5 = []\n",
    "for i in range(len(dataset)):\n",
    "    if dataset[i][0] == rndm_state:\n",
    "        first_5.append(dataset[i])\n",
    "\n",
    "for example in first_5[:1]:\n",
    "    print(f\"Example: {example}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load 8K_T50_model.pt\n",
    "diffusion_model = torch.load(\"10K_T50_model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage Correct:  84.86111111111111 %\n",
      "Sampled Action Stats\n",
      "0: 29.31% (211)\n",
      "1: 42.78% (308)\n",
      "2: 10.69% (77)\n",
      "3: 11.25% (81)\n",
      "4: 3.61% (26)\n",
      "5: 2.36% (17)\n",
      "6: 0.00% (0)\n",
      "7: 0.00% (0)\n",
      "Sum of Percentages:  100.0\n"
     ]
    }
   ],
   "source": [
    "#run experiment sampling 1000 actions from the model and see how many are correct\n",
    "\n",
    "correct_list = []\n",
    "actions_list = []\n",
    "I = 1000\n",
    "discarded = 0\n",
    "for i in range(I):\n",
    "    #sample a random state \n",
    "    rndm_state = torch.randint(0, 500, (1,)).item()\n",
    "\n",
    "    #convert to analog\n",
    "    analog_rndm_state = int2analog(rndm_state, 9)\n",
    "\n",
    "    #sample an action from the model\n",
    "    sampled_action = sample_action(diffusion_model, analog_rndm_state)\n",
    "\n",
    "    #convert to int\n",
    "    int_action = analog2int(torch.sign(sampled_action))\n",
    "\n",
    "\n",
    "    #collect all instances of the state in dataset (could be none)\n",
    "    expert_states = []\n",
    "    for i in range(len(dataset)):\n",
    "        if dataset[i][0] == rndm_state:\n",
    "            expert_states.append(dataset[i])\n",
    "\n",
    "    #if no instance, decrease total and continue\n",
    "    if len(expert_states) == 0:\n",
    "        discarded += 1\n",
    "        continue\n",
    "\n",
    "    #add to actions list\n",
    "    actions_list.append(int_action)\n",
    "\n",
    "    #get most popular expert action for this state\n",
    "    expert_actions = []\n",
    "    for example in expert_states:\n",
    "        expert_actions.append(example[1])\n",
    "    int_expert_action = max(set(expert_actions), key=expert_actions.count)\n",
    "\n",
    "    correct = False\n",
    "    #check if sampled action is correct\n",
    "    if int_action == int_expert_action:\n",
    "        correct_list.append(int_action)\n",
    "        correct = True\n",
    "\n",
    "\n",
    "    # #DEBUG: print to verify manually if correct\n",
    "    # if correct:\n",
    "    #     print(\"CORRECT\")\n",
    "    #     print(\"State: \", rndm_state)\n",
    "    #     print(\"Sampled Action: \", int_action)\n",
    "    #     print(\"Expert Action: \", int_expert_action)\n",
    "    #     print(\"__________\")\n",
    "    \n",
    "\n",
    "\n",
    "#calculate the percentage of correct actions given the states that weren't discarded\n",
    "print(\"Percentage Correct: \", len(correct_list)/(I-discarded) * 100, \"%\")\n",
    "\n",
    "\n",
    "\n",
    "#sampled action statstics (percentage of each action, followed by the number of times it was sampled)\n",
    "print(\"Sampled Action Stats\")\n",
    "for i in range(8):\n",
    "    print(f\"{i}: {(actions_list.count(i)/(I-discarded) * 100):.2f}% ({actions_list.count(i)})\")\n",
    "\n",
    "#add percentages to verify they add up to 100%\n",
    "print(\"Sum of Percentages: \", round(sum([actions_list.count(i)/(I-discarded) * 100 for i in range(8)]), 0))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9.3725e+15, 1.3649e+15, 1.7078e+16])\n",
      "tensor([ 4.2135e+15, -6.3763e+15,  2.1947e+15])\n",
      "tensor([-6.5600e+16,  8.0109e+16,  1.6026e+17])\n",
      "tensor([-7.7759e+16,  8.8545e+16,  1.7272e+17])\n",
      "tensor([1.8158e+16, 1.1176e+17, 1.8795e+17])\n",
      "tensor([1.1821e+16, 3.2379e+16, 3.5294e+16])\n",
      "tensor([-9.3875e+16,  1.3337e+16,  1.0652e+17])\n",
      "tensor([-1.0230e+17, -1.3263e+15,  1.6607e+16])\n",
      "tensor([-5.9111e+16, -2.0804e+16,  9.1314e+16])\n",
      "tensor([-3.2338e+15,  1.8083e+16, -1.0722e+15])\n"
     ]
    }
   ],
   "source": [
    "#example sampled action\n",
    "\n",
    "def sample_test():\n",
    "    #sample a random state\n",
    "    rndm_state = torch.randint(0, 500, (1,)).item()\n",
    "\n",
    "    #convert to analog\n",
    "    analog_rndm_state = int2analog(rndm_state, 9)\n",
    "\n",
    "    #sample an action from the model\n",
    "    sampled_action = sample_action(diffusion_model, analog_rndm_state)\n",
    "\n",
    "    return sampled_action\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print(sample_test())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 25.33% (16605)\n",
      "1: 25.95% (17007)\n",
      "2: 15.95% (10456)\n",
      "3: 17.51% (11476)\n",
      "4: 7.63% (5000)\n",
      "5: 7.63% (5000)\n"
     ]
    }
   ],
   "source": [
    "#collect dataset statistics for analysis\n",
    "\n",
    "#state distribution\n",
    "state_dist = []\n",
    "for i in range(len(dataset)):\n",
    "    state_dist.append(dataset[i][0])\n",
    "\n",
    "#action distribution\n",
    "action_dist = []\n",
    "for i in range(len(dataset)):\n",
    "    action_dist.append(dataset[i][1])\n",
    "\n",
    "\n",
    "#print percentages of each action\n",
    "for i in range(6):\n",
    "    print(f\"{i}: {(action_dist.count(i)/len(action_dist) * 100):.2f}% ({action_dist.count(i)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 25.33% (16605)\n",
      "1: 25.95% (17007)\n",
      "2: 15.95% (10456)\n",
      "3: 17.51% (11476)\n",
      "4: 7.63% (5000)\n",
      "5: 7.63% (5000)\n"
     ]
    }
   ],
   "source": [
    "dataset_mixed = np.loadtxt(path, delimiter=',', skiprows=1, converters={4: lambda x: int(x == 'True')})\n",
    "\n",
    "#state distribution\n",
    "state_dist = []\n",
    "for i in range(len(dataset_mixed)):\n",
    "    state_dist.append(dataset_mixed[i][0])\n",
    "\n",
    "#action distribution\n",
    "action_dist = []\n",
    "for i in range(len(dataset_mixed)):\n",
    "    action_dist.append(dataset_mixed[i][1])\n",
    "\n",
    "\n",
    "#print percentages of each action\n",
    "for i in range(6):\n",
    "    print(f\"{i}: {(action_dist.count(i)/len(action_dist) * 100):.2f}% ({action_dist.count(i)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model to \"4K_T50_model.pt\"\n",
    "torch.save(diffusion_model, \"6K_T50_model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampled action: tensor([-2.0098e+16, -1.7364e+16, -1.7055e+16])\n",
      "thresholded: tensor([-1., -1., -1.])\n",
      "int: 0\n"
     ]
    }
   ],
   "source": [
    "#test loading and sampling from saved model\n",
    "\n",
    "#load model\n",
    "loaded_model = torch.load(\"6K_T50_model.pt\")\n",
    "\n",
    "#sample a random state\n",
    "rndm_state = torch.randint(0, 500, (1,)).item()\n",
    "\n",
    "#convert to analog\n",
    "analog_rndm_state = int2analog(rndm_state, 9)\n",
    "\n",
    "#sample an action from the model\n",
    "sampled_action = sample_action(loaded_model, analog_rndm_state)\n",
    "\n",
    "print(f\"sampled action: {sampled_action}\")\n",
    "print(f\"thresholded: {torch.sign(sampled_action)}\")\n",
    "print(f\"int: {analog2int(torch.sign(sampled_action))}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "loaded_model = torch.load(\"10K_T50_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sampleloop(state_int):\n",
    "    #convert to analog\n",
    "    analog_state = int2analog(state_int, 9)\n",
    "\n",
    "    #sample an action from the model 1000 times\n",
    "    actions = []\n",
    "    action_dict = {0: 0, 1: 0, 2: 0, 3: 0, 4:0, 5:0, 6:0, 7:0}\n",
    "    for i in range(100):\n",
    "        sampled_action = sample_action(loaded_model, analog_state)\n",
    "        actions.append(analog2int(torch.sign(sampled_action)))\n",
    "        action_dict[analog2int(torch.sign(sampled_action))] += 1\n",
    "\n",
    "    return actions, action_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x110 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x110 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract states and actions\n",
    "states = data[:, 0].astype(int)\n",
    "actions = data[:, 1].astype(int)\n",
    "\n",
    "# Find unique states and actions\n",
    "unique_states = np.unique(states)\n",
    "num_actions = len(np.unique(actions))\n",
    "\n",
    "# Initialize matrix to store action frequencies\n",
    "action_frequency = np.zeros((num_actions, 101))\n",
    "\n",
    "# Calculate the frequency of each action for each state\n",
    "for i, state in enumerate(unique_states):\n",
    "    state_actions = actions[states == state]\n",
    "    for action in state_actions:\n",
    "        action_frequency[action, i] += 1\n",
    "\n",
    "    if i == 100:\n",
    "        break\n",
    "\n",
    "# Normalize the frequencies to get probabilities\n",
    "action_frequency /= np.maximum(action_frequency.sum(axis=0), 1)  # Avoid division by zero\n",
    "\n",
    "# Plotting the heatmap using pcolormesh\n",
    "plt.figure(figsize=(20, 1.1))  # Adjust size as needed\n",
    "plt.pcolormesh(action_frequency, cmap='viridis')\n",
    "#shrink y axis (grid is stretched)\n",
    "plt.ylim(0, 6)\n",
    "plt.colorbar(label='Action Frequency')\n",
    "plt.title('Expert Action Distribution')\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Action')\n",
    "plt.yticks(np.arange(0.5, num_actions + 0.5, 1), np.arange(num_actions))\n",
    "plt.xticks([])  # Remove x-ticks\n",
    "plt.show()\n",
    "\n",
    "# Calculate action frequencies for the model\n",
    "model_action_frequency = np.zeros((num_actions, 101))\n",
    "\n",
    "for i, state in enumerate(unique_states):\n",
    "    if i > 100:\n",
    "        break\n",
    "    _, action_dict = sampleloop(state)\n",
    "    for action, count in action_dict.items():\n",
    "        if action > 5:\n",
    "            continue\n",
    "        model_action_frequency[action, i] = count\n",
    "\n",
    "# Normalize the frequencies\n",
    "model_action_frequency /= np.maximum(model_action_frequency.sum(axis=0), 1)\n",
    "\n",
    "# Plotting the heatmap for the model\n",
    "plt.figure(figsize=(20, 1.1))\n",
    "plt.pcolormesh(model_action_frequency, cmap='viridis')\n",
    "plt.ylim(0, 6)\n",
    "plt.colorbar(label='Action Frequency')\n",
    "plt.title('Model Action Distribution')\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Action')\n",
    "plt.yticks(np.arange(0.5, num_actions + 0.5, 1), np.arange(num_actions))\n",
    "plt.xticks([])  # Remove x-ticks\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ambiguous states:  0\n"
     ]
    }
   ],
   "source": [
    "path_medium = 'taxi_q_medium_dataset.csv'\n",
    "\n",
    "medium_data = np.loadtxt(path_medium, delimiter=',', skiprows=1, converters={4: lambda x: int(x == 'True')})\n",
    "\n",
    "states = medium_data[:, 0].astype(int)\n",
    "actions = medium_data[:, 1].astype(int)\n",
    "\n",
    "# create dict for each state state: [actions]\n",
    "state_actions_dict = {}\n",
    "\n",
    "for i, state in enumerate(states):\n",
    "    if state not in state_actions_dict:\n",
    "        state_actions_dict[state] = []\n",
    "    state_actions_dict[state].append(actions[i])\n",
    "\n",
    "# find states with more than 1 action type (ambiguous states)\n",
    "\n",
    "ambiguous_states = []\n",
    "for state, actions in state_actions_dict.items():\n",
    "    if len(set(actions)) > 1:\n",
    "        ambiguous_states.append(state)\n",
    "\n",
    "print(\"Number of ambiguous states: \", len(ambiguous_states))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
