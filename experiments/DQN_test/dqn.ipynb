{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dim: 500, output_dim: 6\n",
      "Episode 0 finished.\n",
      "Episode 10 finished.\n",
      "Episode 20 finished.\n",
      "Episode 30 finished.\n",
      "Episode 40 finished.\n",
      "Episode 50 finished.\n",
      "Episode 60 finished.\n",
      "Episode 70 finished.\n",
      "Episode 80 finished.\n",
      "Episode 90 finished.\n",
      "Episode 100 finished.\n",
      "Episode 110 finished.\n",
      "Episode 120 finished.\n",
      "Episode 130 finished.\n",
      "Episode 140 finished.\n",
      "Episode 150 finished.\n",
      "Episode 160 finished.\n",
      "Episode 170 finished.\n",
      "Episode 180 finished.\n",
      "Episode 190 finished.\n",
      "Episode 200 finished.\n",
      "Episode 210 finished.\n",
      "Episode 220 finished.\n",
      "Episode 230 finished.\n",
      "Episode 240 finished.\n",
      "Episode 250 finished.\n",
      "Episode 260 finished.\n",
      "Episode 270 finished.\n",
      "Episode 280 finished.\n",
      "Episode 290 finished.\n",
      "Episode 300 finished.\n",
      "Episode 310 finished.\n",
      "Episode 320 finished.\n",
      "Episode 330 finished.\n",
      "Episode 340 finished.\n",
      "Episode 350 finished.\n",
      "Episode 360 finished.\n",
      "Episode 370 finished.\n",
      "Episode 380 finished.\n",
      "Episode 390 finished.\n",
      "Episode 400 finished.\n",
      "Episode 410 finished.\n",
      "Episode 420 finished.\n",
      "Episode 430 finished.\n",
      "Episode 440 finished.\n",
      "Episode 450 finished.\n",
      "Episode 460 finished.\n",
      "Episode 470 finished.\n",
      "Episode 480 finished.\n",
      "Episode 490 finished.\n",
      "Episode 500 finished.\n",
      "Episode 510 finished.\n",
      "Episode 520 finished.\n",
      "Episode 530 finished.\n",
      "Episode 540 finished.\n",
      "Episode 550 finished.\n",
      "Episode 560 finished.\n",
      "Episode 570 finished.\n",
      "Episode 580 finished.\n",
      "Episode 590 finished.\n",
      "Episode 600 finished.\n",
      "Episode 610 finished.\n",
      "Episode 620 finished.\n",
      "Episode 630 finished.\n",
      "Episode 640 finished.\n",
      "Episode 650 finished.\n",
      "Episode 660 finished.\n",
      "Episode 670 finished.\n",
      "Episode 680 finished.\n",
      "Episode 690 finished.\n",
      "Episode 700 finished.\n",
      "Episode 710 finished.\n",
      "Episode 720 finished.\n",
      "Episode 730 finished.\n",
      "Episode 740 finished.\n",
      "Episode 750 finished.\n",
      "Episode 760 finished.\n",
      "Episode 770 finished.\n",
      "Episode 780 finished.\n",
      "Episode 790 finished.\n",
      "Episode 800 finished.\n",
      "Episode 810 finished.\n",
      "Episode 820 finished.\n",
      "Episode 830 finished.\n",
      "Episode 840 finished.\n",
      "Episode 850 finished.\n",
      "Episode 860 finished.\n",
      "Episode 870 finished.\n",
      "Episode 880 finished.\n",
      "Episode 890 finished.\n",
      "Episode 900 finished.\n",
      "Episode 910 finished.\n",
      "Episode 920 finished.\n",
      "Episode 930 finished.\n",
      "Episode 940 finished.\n",
      "Episode 950 finished.\n",
      "Episode 960 finished.\n",
      "Episode 970 finished.\n",
      "Episode 980 finished.\n",
      "Episode 990 finished.\n",
      "Episode 1000 finished.\n",
      "Episode 1010 finished.\n",
      "Episode 1020 finished.\n",
      "Episode 1030 finished.\n",
      "Episode 1040 finished.\n",
      "Episode 1050 finished.\n",
      "Episode 1060 finished.\n",
      "Episode 1070 finished.\n",
      "Episode 1080 finished.\n",
      "Episode 1090 finished.\n",
      "Episode 1100 finished.\n",
      "Episode 1110 finished.\n",
      "Episode 1120 finished.\n",
      "Episode 1130 finished.\n",
      "Episode 1140 finished.\n",
      "Episode 1150 finished.\n",
      "Episode 1160 finished.\n",
      "Episode 1170 finished.\n",
      "Episode 1180 finished.\n",
      "Episode 1190 finished.\n",
      "Episode 1200 finished.\n",
      "Episode 1210 finished.\n",
      "Episode 1220 finished.\n",
      "Episode 1230 finished.\n",
      "Episode 1240 finished.\n",
      "Episode 1250 finished.\n",
      "Episode 1260 finished.\n",
      "Episode 1270 finished.\n",
      "Episode 1280 finished.\n",
      "Episode 1290 finished.\n",
      "Episode 1300 finished.\n",
      "Episode 1310 finished.\n",
      "Episode 1320 finished.\n",
      "Episode 1330 finished.\n",
      "Episode 1340 finished.\n",
      "Episode 1350 finished.\n",
      "Episode 1360 finished.\n",
      "Episode 1370 finished.\n",
      "Episode 1380 finished.\n",
      "Episode 1390 finished.\n",
      "Episode 1400 finished.\n",
      "Episode 1410 finished.\n",
      "Episode 1420 finished.\n",
      "Episode 1430 finished.\n",
      "Episode 1440 finished.\n",
      "Episode 1450 finished.\n",
      "Episode 1460 finished.\n",
      "Episode 1470 finished.\n",
      "Episode 1480 finished.\n",
      "Episode 1490 finished.\n",
      "Episode 1500 finished.\n",
      "Episode 1510 finished.\n",
      "Episode 1520 finished.\n",
      "Episode 1530 finished.\n",
      "Episode 1540 finished.\n",
      "Episode 1550 finished.\n",
      "Episode 1560 finished.\n",
      "Episode 1570 finished.\n",
      "Episode 1580 finished.\n",
      "Episode 1590 finished.\n",
      "Episode 1600 finished.\n",
      "Episode 1610 finished.\n",
      "Episode 1620 finished.\n",
      "Episode 1630 finished.\n",
      "Episode 1640 finished.\n",
      "Episode 1650 finished.\n",
      "Episode 1660 finished.\n",
      "Episode 1670 finished.\n",
      "Episode 1680 finished.\n",
      "Episode 1690 finished.\n",
      "Episode 1700 finished.\n",
      "Episode 1710 finished.\n",
      "Episode 1720 finished.\n",
      "Episode 1730 finished.\n",
      "Episode 1740 finished.\n",
      "Episode 1750 finished.\n",
      "Episode 1760 finished.\n",
      "Episode 1770 finished.\n",
      "Episode 1780 finished.\n",
      "Episode 1790 finished.\n",
      "Episode 1800 finished.\n",
      "Episode 1810 finished.\n",
      "Episode 1820 finished.\n",
      "Episode 1830 finished.\n",
      "Episode 1840 finished.\n",
      "Episode 1850 finished.\n",
      "Episode 1860 finished.\n",
      "Episode 1870 finished.\n",
      "Episode 1880 finished.\n",
      "Episode 1890 finished.\n",
      "Episode 1900 finished.\n",
      "Episode 1910 finished.\n",
      "Episode 1920 finished.\n",
      "Episode 1930 finished.\n",
      "Episode 1940 finished.\n",
      "Episode 1950 finished.\n",
      "Episode 1960 finished.\n",
      "Episode 1970 finished.\n",
      "Episode 1980 finished.\n",
      "Episode 1990 finished.\n",
      "Episode 2000 finished.\n",
      "Episode 2010 finished.\n",
      "Episode 2020 finished.\n",
      "Episode 2030 finished.\n",
      "Episode 2040 finished.\n",
      "Episode 2050 finished.\n",
      "Episode 2060 finished.\n",
      "Episode 2070 finished.\n",
      "Episode 2080 finished.\n",
      "Episode 2090 finished.\n",
      "Episode 2100 finished.\n",
      "Episode 2110 finished.\n",
      "Episode 2120 finished.\n",
      "Episode 2130 finished.\n",
      "Episode 2140 finished.\n",
      "Episode 2150 finished.\n",
      "Episode 2160 finished.\n",
      "Episode 2170 finished.\n",
      "Episode 2180 finished.\n",
      "Episode 2190 finished.\n",
      "Episode 2200 finished.\n",
      "Episode 2210 finished.\n",
      "Episode 2220 finished.\n",
      "Episode 2230 finished.\n",
      "Episode 2240 finished.\n",
      "Episode 2250 finished.\n",
      "Episode 2260 finished.\n",
      "Episode 2270 finished.\n",
      "Episode 2280 finished.\n",
      "Episode 2290 finished.\n",
      "Episode 2300 finished.\n",
      "Episode 2310 finished.\n",
      "Episode 2320 finished.\n",
      "Episode 2330 finished.\n",
      "Episode 2340 finished.\n",
      "Episode 2350 finished.\n",
      "Episode 2360 finished.\n",
      "Episode 2370 finished.\n",
      "Episode 2380 finished.\n",
      "Episode 2390 finished.\n",
      "Episode 2400 finished.\n",
      "Episode 2410 finished.\n",
      "Episode 2420 finished.\n",
      "Episode 2430 finished.\n",
      "Episode 2440 finished.\n",
      "Episode 2450 finished.\n",
      "Episode 2460 finished.\n",
      "Episode 2470 finished.\n",
      "Episode 2480 finished.\n",
      "Episode 2490 finished.\n",
      "Episode 2500 finished.\n",
      "Episode 2510 finished.\n",
      "Episode 2520 finished.\n",
      "Episode 2530 finished.\n",
      "Episode 2540 finished.\n",
      "Episode 2550 finished.\n",
      "Episode 2560 finished.\n",
      "Episode 2570 finished.\n",
      "Episode 2580 finished.\n",
      "Episode 2590 finished.\n",
      "Episode 2600 finished.\n",
      "Episode 2610 finished.\n",
      "Episode 2620 finished.\n",
      "Episode 2630 finished.\n",
      "Episode 2640 finished.\n",
      "Episode 2650 finished.\n",
      "Episode 2660 finished.\n",
      "Episode 2670 finished.\n",
      "Episode 2680 finished.\n",
      "Episode 2690 finished.\n",
      "Episode 2700 finished.\n",
      "Episode 2710 finished.\n",
      "Episode 2720 finished.\n",
      "Episode 2730 finished.\n",
      "Episode 2740 finished.\n",
      "Episode 2750 finished.\n",
      "Episode 2760 finished.\n",
      "Episode 2770 finished.\n",
      "Episode 2780 finished.\n",
      "Episode 2790 finished.\n",
      "Episode 2800 finished.\n",
      "Episode 2810 finished.\n",
      "Episode 2820 finished.\n",
      "Episode 2830 finished.\n",
      "Episode 2840 finished.\n",
      "Episode 2850 finished.\n",
      "Episode 2860 finished.\n",
      "Episode 2870 finished.\n",
      "Episode 2880 finished.\n",
      "Episode 2890 finished.\n",
      "Episode 2900 finished.\n",
      "Episode 2910 finished.\n",
      "Episode 2920 finished.\n",
      "Episode 2930 finished.\n",
      "Episode 2940 finished.\n",
      "Episode 2950 finished.\n",
      "Episode 2960 finished.\n",
      "Episode 2970 finished.\n",
      "Episode 2980 finished.\n",
      "Episode 2990 finished.\n",
      "Episode 3000 finished.\n",
      "Episode 3010 finished.\n",
      "Episode 3020 finished.\n",
      "Episode 3030 finished.\n",
      "Episode 3040 finished.\n",
      "Episode 3050 finished.\n",
      "Episode 3060 finished.\n",
      "Episode 3070 finished.\n",
      "Episode 3080 finished.\n",
      "Episode 3090 finished.\n",
      "Episode 3100 finished.\n",
      "Episode 3110 finished.\n",
      "Episode 3120 finished.\n",
      "Episode 3130 finished.\n",
      "Episode 3140 finished.\n",
      "Episode 3150 finished.\n",
      "Episode 3160 finished.\n",
      "Episode 3170 finished.\n",
      "Episode 3180 finished.\n",
      "Episode 3190 finished.\n",
      "Episode 3200 finished.\n",
      "Episode 3210 finished.\n",
      "Episode 3220 finished.\n",
      "Episode 3230 finished.\n",
      "Episode 3240 finished.\n",
      "Episode 3250 finished.\n",
      "Episode 3260 finished.\n",
      "Episode 3270 finished.\n",
      "Episode 3280 finished.\n",
      "Episode 3290 finished.\n",
      "Episode 3300 finished.\n",
      "Episode 3310 finished.\n",
      "Episode 3320 finished.\n",
      "Episode 3330 finished.\n",
      "Episode 3340 finished.\n",
      "Episode 3350 finished.\n",
      "Episode 3360 finished.\n",
      "Episode 3370 finished.\n",
      "Episode 3380 finished.\n",
      "Episode 3390 finished.\n",
      "Episode 3400 finished.\n",
      "Episode 3410 finished.\n",
      "Episode 3420 finished.\n",
      "Episode 3430 finished.\n",
      "Episode 3440 finished.\n",
      "Episode 3450 finished.\n",
      "Episode 3460 finished.\n",
      "Episode 3470 finished.\n",
      "Episode 3480 finished.\n",
      "Episode 3490 finished.\n",
      "Episode 3500 finished.\n",
      "Episode 3510 finished.\n",
      "Episode 3520 finished.\n",
      "Episode 3530 finished.\n",
      "Episode 3540 finished.\n",
      "Episode 3550 finished.\n",
      "Episode 3560 finished.\n",
      "Episode 3570 finished.\n",
      "Episode 3580 finished.\n",
      "Episode 3590 finished.\n",
      "Episode 3600 finished.\n",
      "Episode 3610 finished.\n",
      "Episode 3620 finished.\n",
      "Episode 3630 finished.\n",
      "Episode 3640 finished.\n",
      "Episode 3650 finished.\n",
      "Episode 3660 finished.\n",
      "Episode 3670 finished.\n",
      "Episode 3680 finished.\n",
      "Episode 3690 finished.\n",
      "Episode 3700 finished.\n",
      "Episode 3710 finished.\n",
      "Episode 3720 finished.\n",
      "Episode 3730 finished.\n",
      "Episode 3740 finished.\n",
      "Episode 3750 finished.\n",
      "Episode 3760 finished.\n",
      "Episode 3770 finished.\n",
      "Episode 3780 finished.\n",
      "Episode 3790 finished.\n",
      "Episode 3800 finished.\n",
      "Episode 3810 finished.\n",
      "Episode 3820 finished.\n",
      "Episode 3830 finished.\n",
      "Episode 3840 finished.\n",
      "Episode 3850 finished.\n",
      "Episode 3860 finished.\n",
      "Episode 3870 finished.\n",
      "Episode 3880 finished.\n",
      "Episode 3890 finished.\n",
      "Episode 3900 finished.\n",
      "Episode 3910 finished.\n",
      "Episode 3920 finished.\n",
      "Episode 3930 finished.\n",
      "Episode 3940 finished.\n",
      "Episode 3950 finished.\n",
      "Episode 3960 finished.\n",
      "Episode 3970 finished.\n",
      "Episode 3980 finished.\n",
      "Episode 3990 finished.\n",
      "Episode 4000 finished.\n",
      "Episode 4010 finished.\n",
      "Episode 4020 finished.\n",
      "Episode 4030 finished.\n",
      "Episode 4040 finished.\n",
      "Episode 4050 finished.\n",
      "Episode 4060 finished.\n",
      "Episode 4070 finished.\n",
      "Episode 4080 finished.\n",
      "Episode 4090 finished.\n",
      "Episode 4100 finished.\n",
      "Episode 4110 finished.\n",
      "Episode 4120 finished.\n",
      "Episode 4130 finished.\n",
      "Episode 4140 finished.\n",
      "Episode 4150 finished.\n",
      "Episode 4160 finished.\n",
      "Episode 4170 finished.\n",
      "Episode 4180 finished.\n",
      "Episode 4190 finished.\n",
      "Episode 4200 finished.\n",
      "Episode 4210 finished.\n",
      "Episode 4220 finished.\n",
      "Episode 4230 finished.\n",
      "Episode 4240 finished.\n",
      "Episode 4250 finished.\n",
      "Episode 4260 finished.\n",
      "Episode 4270 finished.\n",
      "Episode 4280 finished.\n",
      "Episode 4290 finished.\n",
      "Episode 4300 finished.\n",
      "Episode 4310 finished.\n",
      "Episode 4320 finished.\n",
      "Episode 4330 finished.\n",
      "Episode 4340 finished.\n",
      "Episode 4350 finished.\n",
      "Episode 4360 finished.\n",
      "Episode 4370 finished.\n",
      "Episode 4380 finished.\n",
      "Episode 4390 finished.\n",
      "Episode 4400 finished.\n",
      "Episode 4410 finished.\n",
      "Episode 4420 finished.\n",
      "Episode 4430 finished.\n",
      "Episode 4440 finished.\n",
      "Episode 4450 finished.\n",
      "Episode 4460 finished.\n",
      "Episode 4470 finished.\n",
      "Episode 4480 finished.\n",
      "Episode 4490 finished.\n",
      "Episode 4500 finished.\n",
      "Episode 4510 finished.\n",
      "Episode 4520 finished.\n",
      "Episode 4530 finished.\n",
      "Episode 4540 finished.\n",
      "Episode 4550 finished.\n",
      "Episode 4560 finished.\n",
      "Episode 4570 finished.\n",
      "Episode 4580 finished.\n",
      "Episode 4590 finished.\n",
      "Episode 4600 finished.\n",
      "Episode 4610 finished.\n",
      "Episode 4620 finished.\n",
      "Episode 4630 finished.\n",
      "Episode 4640 finished.\n",
      "Episode 4650 finished.\n",
      "Episode 4660 finished.\n",
      "Episode 4670 finished.\n",
      "Episode 4680 finished.\n",
      "Episode 4690 finished.\n",
      "Episode 4700 finished.\n",
      "Episode 4710 finished.\n",
      "Episode 4720 finished.\n",
      "Episode 4730 finished.\n",
      "Episode 4740 finished.\n",
      "Episode 4750 finished.\n",
      "Episode 4760 finished.\n",
      "Episode 4770 finished.\n",
      "Episode 4780 finished.\n",
      "Episode 4790 finished.\n",
      "Episode 4800 finished.\n",
      "Episode 4810 finished.\n",
      "Episode 4820 finished.\n",
      "Episode 4830 finished.\n",
      "Episode 4840 finished.\n",
      "Episode 4850 finished.\n",
      "Episode 4860 finished.\n",
      "Episode 4870 finished.\n",
      "Episode 4880 finished.\n",
      "Episode 4890 finished.\n",
      "Episode 4900 finished.\n",
      "Episode 4910 finished.\n",
      "Episode 4920 finished.\n",
      "Episode 4930 finished.\n",
      "Episode 4940 finished.\n",
      "Episode 4950 finished.\n",
      "Episode 4960 finished.\n",
      "Episode 4970 finished.\n",
      "Episode 4980 finished.\n",
      "Episode 4990 finished.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gym\n",
    "from collections import deque\n",
    "from random import sample\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "def one_hot_state(state, n_states):\n",
    "    vec = np.zeros(n_states)\n",
    "    vec[state] = 1\n",
    "    return vec\n",
    "\n",
    "# Constants\n",
    "REPLAY_BUFFER_SIZE = 10000\n",
    "MIN_REPLAY_BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Initialize replay buffer\n",
    "replay_buffer = deque(maxlen=10000)\n",
    "\n",
    "def train_dqn(env, model, episodes=5000, learning_rate=0.001, discount_factor=0.95, exploration_prob=1.0, exploration_decay=0.995, min_exploration=0.05):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    n_states = env.observation_space.n\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = one_hot_state(env.reset()[0], n_states)\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        while not terminated and not truncated:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.tensor([state], dtype=torch.float32)\n",
    "                if np.random.uniform(0, 1) < exploration_prob:\n",
    "                    action = env.action_space.sample()  # Explore\n",
    "                else:\n",
    "                    q_values = model(state_tensor)\n",
    "                    action = torch.argmax(q_values).item()  # Exploit\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            next_state = one_hot_state(next_state, n_states)\n",
    "\n",
    "            # Store experience in replay buffer\n",
    "            replay_buffer.append((state, action, reward, next_state, terminated))\n",
    "            if len(replay_buffer) > REPLAY_BUFFER_SIZE:\n",
    "                replay_buffer.pop(0)  # Remove oldest experience if buffer is full\n",
    "\n",
    "            # Train using experience replay\n",
    "            if len(replay_buffer) >= MIN_REPLAY_BUFFER_SIZE:\n",
    "                batch = sample(replay_buffer, BATCH_SIZE)\n",
    "                states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "                states = torch.tensor(states, dtype=torch.float32)\n",
    "                actions = torch.tensor(actions, dtype=torch.long)\n",
    "                rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "                next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "                dones = torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "                q_values = model(states)\n",
    "                q_values = q_values.gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    next_q_values = model(next_states)\n",
    "                    next_q_values = torch.max(next_q_values, dim=1)[0]\n",
    "                    targets = rewards + (1 - dones) * discount_factor * next_q_values\n",
    "\n",
    "                loss = loss_fn(q_values, targets)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        exploration_prob = max(min_exploration, exploration_prob * exploration_decay)\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episode {episode} finished.\")\n",
    "\n",
    "\n",
    "# Train\n",
    "env = gym.make('Taxi-v3')\n",
    "input_dim = env.observation_space.n  # Adjusted to match the number of states\n",
    "output_dim = env.action_space.n\n",
    "print(f\"input_dim: {input_dim}, output_dim: {output_dim}\")\n",
    "model = DQN(input_dim, output_dim)\n",
    "train_dqn(env, model)\n",
    "\n",
    "#save model\n",
    "torch.save(model, 'dqn-model1.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: 500\n",
      "Debug - next_state: 134\n",
      "Debug - state: [134]\n",
      "Debug - action: 4\n",
      "Debug - target: tensor(108.1784)\n",
      "Debug - next_state: 134\n",
      "Debug - state: 134\n",
      "Debug - action: 4\n",
      "Debug - target: tensor(108.0502)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of bounds for dimension 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/student/d2ql/honours-thesis/datasets/DQN_test/dqn.ipynb Cell 6\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu1/home/student/d2ql/honours-thesis/datasets/DQN_test/dqn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m output_dim \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mn\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu1/home/student/d2ql/honours-thesis/datasets/DQN_test/dqn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m model \u001b[39m=\u001b[39m DQN(input_dim, output_dim)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu1/home/student/d2ql/honours-thesis/datasets/DQN_test/dqn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m train_dqn(env, model)\n",
      "\u001b[1;32m/home/student/d2ql/honours-thesis/datasets/DQN_test/dqn.ipynb Cell 6\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu1/home/student/d2ql/honours-thesis/datasets/DQN_test/dqn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDebug - target:\u001b[39m\u001b[39m\"\u001b[39m, target)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu1/home/student/d2ql/honours-thesis/datasets/DQN_test/dqn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m q_values \u001b[39m=\u001b[39m model(torch\u001b[39m.\u001b[39mtensor([state], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32))\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu1/home/student/d2ql/honours-thesis/datasets/DQN_test/dqn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(q_values[\u001b[39m0\u001b[39;49m][action], torch\u001b[39m.\u001b[39mtensor([target], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu1/home/student/d2ql/honours-thesis/datasets/DQN_test/dqn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu1/home/student/d2ql/honours-thesis/datasets/DQN_test/dqn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "\u001b[0;31mIndexError\u001b[0m: index 4 is out of bounds for dimension 0 with size 0"
     ]
    }
   ],
   "source": [
    "def train_dqn(env, model, episodes=5000, learning_rate=0.001, discount_factor=0.95, exploration_prob=1.0, exploration_decay=0.995, min_exploration=0.05):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        state = np.array([state])\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        while not terminated and not truncated:\n",
    "            with torch.no_grad():\n",
    "                if np.random.uniform(0, 1) < exploration_prob:\n",
    "                    action = env.action_space.sample()  # Explore\n",
    "                else:\n",
    "                    q_values = model(torch.tensor([state], dtype=torch.float32))\n",
    "                    print(\"Debug - q_values shape:\", q_values.shape)\n",
    "                    print(\"Debug - q_values:\", q_values)\n",
    "                    action = torch.argmax(q_values).item()  # Exploit\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            print(\"Debug - next_state:\", next_state)\n",
    "            print(\"Debug - state:\", state)\n",
    "            print(\"Debug - action:\", action)\n",
    "\n",
    "            # Update model\n",
    "            target = reward\n",
    "            if not terminated and not truncated:\n",
    "                with torch.no_grad():\n",
    "                    target = reward + discount_factor * torch.max(model(torch.tensor([next_state], dtype=torch.float32)))\n",
    "                    print(\"Debug - target:\", target)\n",
    "\n",
    "            q_values = model(torch.tensor([state], dtype=torch.float32))\n",
    "           \n",
    "            loss = loss_fn(q_values[0][action], torch.tensor([target], dtype=torch.float32))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        exploration_prob = max(min_exploration, exploration_prob * exploration_decay)\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode {episode} finished.\")\n",
    "\n",
    "# Train\n",
    "env = gym.make('Taxi-v3')\n",
    "print(\"Observation space:\", env.observation_space.n)\n",
    "input_dim = 1\n",
    "output_dim = env.action_space.n\n",
    "model = DQN(input_dim, output_dim)\n",
    "train_dqn(env, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
